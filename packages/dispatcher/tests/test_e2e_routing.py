"""End-to-end routing tests for the dispatcher.

Messages flow through the REAL dispatcher pipeline:
  _on_message() â†’ _classify() (real LLM API call) â†’ handler

But Telegram and Claude CLI are mocked to capture behavior
without side effects. This tests the actual routing decisions.

Usage:
    python3 -m tests.test_e2e_routing
"""

from __future__ import annotations

import asyncio
import json
import sys
import time
import uuid
from dataclasses import dataclass, field
from pathlib import Path
from unittest.mock import MagicMock, AsyncMock, patch

sys.path.insert(0, str(Path(__file__).resolve().parent.parent))

from dispatcher.config import Config, DEFAULTS, _deep_copy
from dispatcher.core import Dispatcher
from dispatcher.session import Session, SessionManager


# â”€â”€ Mock components â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

class MockTelegram:
    """Captures all outgoing Telegram calls for inspection."""

    def __init__(self):
        self.sent: list[dict] = []
        self.edits: list[dict] = []
        self.reactions: list[dict] = []
        self.typing_count = 0
        self._msg_counter = 1000

    def send(self, text, reply_to=None, parse_mode=None, reply_markup=None):
        self._msg_counter += 1
        self.sent.append({
            "text": text, "reply_to": reply_to,
            "parse_mode": parse_mode, "reply_markup": reply_markup,
            "msg_id": self._msg_counter,
        })
        return self._msg_counter

    def edit(self, message_id, text, parse_mode=None):
        self.edits.append({"message_id": message_id, "text": text})

    def typing(self):
        self.typing_count += 1

    def react(self, message_id, emoji):
        self.reactions.append({"message_id": message_id, "emoji": emoji})

    def set_my_commands(self, commands):
        pass

    def answer_callback(self, cb_id, text=""):
        pass

    def send_document(self, path, caption="", reply_to=None):
        self.sent.append({"document": path, "caption": caption, "reply_to": reply_to})
        self._msg_counter += 1
        return self._msg_counter

    def download_file(self, file_id, path):
        return False

    def get_file_url(self, file_id):
        return None

    def send_photo(self, path, caption="", reply_to=None):
        pass

    def poll(self, offset, timeout):
        return []

    def reset(self):
        self.sent.clear()
        self.edits.clear()
        self.reactions.clear()
        self.typing_count = 0


class MockRunner:
    """Captures Claude CLI invocations without running them."""

    def __init__(self):
        self.invocations: list[dict] = []

    async def invoke(self, session, prompt, resume=False, max_turns=50,
                     model=None, stream=True):
        self.invocations.append({
            "sid": session.sid[:8],
            "prompt_preview": prompt[:100],
            "resume": resume,
            "max_turns": max_turns,
            "model": model,
            "stream": stream,
            "cwd": session.cwd,
        })
        session.status = "done"
        session.finished = time.time()
        session.result = f"[mock result for: {session.task_text[:50]}]"
        return session.result

    def reset(self):
        self.invocations.clear()


# â”€â”€ Test harness â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

def make_config():
    """Create a minimal config for testing."""
    data = _deep_copy(DEFAULTS)
    data["telegram"]["bot_token"] = "test-token"
    data["telegram"]["chat_id"] = 12345
    data["projects"] = {
        "sims": {"path": "/tmp/test-sims", "keywords": ["sims", "titan"]},
        "cortex": {"path": "/tmp/test-cortex", "keywords": ["cortex", "dispatcher"]},
    }
    cfg = Config.__new__(Config)
    cfg._data = data
    cfg._path = Path("/dev/null")
    return cfg


def make_dispatcher():
    """Create a dispatcher with mocked I/O."""
    cfg = make_config()
    d = Dispatcher(cfg)
    d.tg = MockTelegram()
    d.runner = MockRunner()
    return d


def make_message(text: str, mid: int = None, reply_to: int = None,
                 chat_id: int = 12345) -> dict:
    """Create a minimal Telegram message dict."""
    return {
        "message_id": mid or int(time.time() * 1000) % 100000,
        "chat": {"id": chat_id},
        "text": text,
        "reply_to_message": {"message_id": reply_to} if reply_to else None,
    }


def add_running_session(d: Dispatcher, project: str = "sims",
                        task: str = "running data analysis",
                        elapsed_seconds: int = 300) -> Session:
    """Add a fake running session to the dispatcher."""
    mid = int(time.time() * 1000) % 100000 + 500
    s = d.sm.create(mid, task, f"/tmp/test-{project}")
    s.status = "running"
    s.started = time.time() - elapsed_seconds
    s.partial_output = "Processing data files...\nAnalyzing correlations..."
    # Mock process so cancel works
    s.proc = MagicMock()
    s.proc.kill = MagicMock()
    return s


@dataclass
class E2EResult:
    scenario: str
    message: str
    expected_route: str
    actual_route: str
    passed: bool
    latency_ms: float
    details: str = ""
    tags: list[str] = field(default_factory=list)


def detect_route(d: Dispatcher, tg: MockTelegram, runner: MockRunner) -> str:
    """Detect which handler was triggered based on mock state."""
    if runner.invocations:
        inv = runner.invocations[-1]
        if inv.get("model") == "haiku" and inv.get("max_turns") == 3:
            return "quick"
        return "task"

    if not tg.sent:
        return "unknown"

    last_text = tg.sent[-1].get("text", "")

    # Detect handler by response content
    if "ç©ºé—²ä¸­" in last_text or "æ­£åœ¨è¿è¡Œ" in last_text or "å½“å‰è¾“å‡º:" in last_text:
        if "å½“å‰è¾“å‡º:" in last_text or "æš‚æ— è¾“å‡º" in last_text:
            return "peek"
        return "status"
    if "å·²å–æ¶ˆ" in last_text or "æ²¡æœ‰åœ¨è·‘" in last_text or "ä¸ç¡®å®šè¦å–æ¶ˆ" in last_text:
        return "cancel"
    if "å½“å‰è¾“å‡º" in last_text or "æš‚æ— è¾“å‡º" in last_text:
        return "peek"
    if "æœ€è¿‘ä»»åŠ¡" in last_text or "æ²¡æœ‰æœ€è¿‘å®Œæˆ" in last_text:
        return "history"
    if "ä½¿ç”¨æŒ‡å—" in last_text:
        return "help"
    if "æ–°å¯¹è¯" in last_text or "æ–°å»ºsession" in last_text.lower():
        return "new_session"
    if "ç”¨æ³•: /q" in last_text:
        return "quick"

    # If runner was invoked, it's a task
    return "unknown"


# â”€â”€ Test scenarios â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

@dataclass
class Scenario:
    name: str
    message: str
    expected: str
    setup: str = "running"  # "running", "idle", "multi"
    tags: list[str] = field(default_factory=list)
    reply_to: int | None = None


SCENARIOS = [
    # â”€â”€ Status queries while task is running â”€â”€
    Scenario("status_chinese_1", "è¿˜åœ¨è·‘å—", "status", tags=["status"]),
    Scenario("status_chinese_2", "è¿›åº¦å¦‚ä½•", "status", tags=["status"]),
    Scenario("status_chinese_3", "è·‘å®Œäº†å—", "status", tags=["status"]),
    Scenario("status_chinese_4", "æžå®šäº†å—", "status", tags=["status"]),
    Scenario("status_chinese_5", "è¿˜è¦å¤šä¹…", "status", tags=["status"]),
    Scenario("status_english_1", "still running?", "status", tags=["status"]),
    Scenario("status_english_2", "is it done yet", "status", tags=["status"]),
    Scenario("status_english_3", "any progress?", "status", tags=["status"]),
    Scenario("status_natural", "é‚£ä¸ªä»»åŠ¡è·‘å¾—æ€Žä¹ˆæ ·äº†", "status", tags=["status"]),
    Scenario("status_impatient", "è¿™éƒ½è·‘äº†å¥½ä¹…äº†è¿˜æ²¡å¥½ï¼Ÿ", "status", tags=["status"]),

    # â”€â”€ Cancel requests while task is running â”€â”€
    Scenario("cancel_chinese_1", "killæŽ‰å§", "cancel", tags=["cancel"]),
    Scenario("cancel_chinese_2", "åˆ«è·‘äº†", "cancel", tags=["cancel"]),
    Scenario("cancel_chinese_3", "å–æ¶ˆå§", "cancel", tags=["cancel"]),
    Scenario("cancel_chinese_4", "å¤ªæ…¢äº†ä¸è¦äº†", "cancel", tags=["cancel"]),
    Scenario("cancel_english_1", "stop it", "cancel", tags=["cancel"]),
    Scenario("cancel_english_2", "kill it", "cancel", tags=["cancel"]),
    Scenario("cancel_english_3", "abort", "cancel", tags=["cancel"]),
    Scenario("cancel_combo", "å¤ªæ…¢äº† killæŽ‰å§ ä¸è¦äº†", "cancel", tags=["cancel"]),

    # â”€â”€ Peek at output while task is running â”€â”€
    Scenario("peek_chinese_1", "çœ‹çœ‹è¾“å‡º", "peek", tags=["peek"]),
    Scenario("peek_chinese_2", "ç»™æˆ‘çœ‹çœ‹å®ƒçŽ°åœ¨å†™äº†ä»€ä¹ˆ", "peek", tags=["peek"]),
    Scenario("peek_english_1", "show me the output", "peek", tags=["peek"]),
    Scenario("peek_natural", "è®©æˆ‘çœ‹çœ‹ä½ çŽ°åœ¨å†™äº†å•¥", "peek", tags=["peek"]),

    # â”€â”€ Tasks that should NOT be intercepted â”€â”€
    Scenario("task_project_check", "å¸®æˆ‘æ£€æŸ¥ä¸€ä¸‹proactiveé¡¹ç›®çš„è¿›åº¦", "task", tags=["task", "critical"]),
    Scenario("task_code_write", "å¸®æˆ‘å†™ä¸ªPythonè„šæœ¬", "task", tags=["task"]),
    Scenario("task_bug_fix", "ä¿®å¤é‚£ä¸ªbug", "task", tags=["task"]),
    Scenario("task_git", "git pushä¸€ä¸‹", "task", tags=["task"]),
    Scenario("task_tests", "æŠŠæµ‹è¯•è·‘ä¸€ä¸‹", "task", tags=["task"]),
    Scenario("task_english", "run the tests", "task", tags=["task"]),
    Scenario("task_refactor", "refactor that function", "task", tags=["task"]),
    Scenario("task_readme", "å¸®æˆ‘æ›´æ–°README", "task", tags=["task"]),
    Scenario("task_kill_specific", "å¸®æˆ‘killæŽ‰é‚£ä¸ªåƒµå°¸è¿›ç¨‹", "task", tags=["task", "critical"]),
    Scenario("task_stop_docker", "stop the docker container", "task", tags=["task", "critical"]),
    Scenario("task_cancel_cron", "å¸®æˆ‘å–æ¶ˆé‚£ä¸ªscheduled job", "task", tags=["task", "critical"]),
    Scenario("task_check_status", "çœ‹çœ‹é‚£ä¸ªæœåŠ¡çš„è¿è¡ŒçŠ¶æ€", "task", tags=["task"]),

    # â”€â”€ /commands (instant, no LLM) â”€â”€
    Scenario("cmd_status", "/status", "status", tags=["command"]),
    Scenario("cmd_cancel", "/cancel", "cancel", tags=["command"]),
    Scenario("cmd_peek", "/peek", "peek", tags=["command"]),
    Scenario("cmd_help", "/help", "help", tags=["command"]),
    Scenario("cmd_history", "/history", "history", tags=["command"]),
    Scenario("cmd_quick", "/q what is a monad", "quick", tags=["command"]),

    # â”€â”€ Idle state (no running tasks) â€” everything is task â”€â”€
    Scenario("idle_status_word", "è¿›åº¦å¦‚ä½•", "task", setup="idle", tags=["idle"]),
    Scenario("idle_cancel_word", "å–æ¶ˆ", "task", setup="idle", tags=["idle"]),
    Scenario("idle_normal", "å¸®æˆ‘å†™ä¸ªè„šæœ¬", "task", setup="idle", tags=["idle"]),

    # â”€â”€ Multiple running tasks â”€â”€
    Scenario("multi_status", "è¿˜åœ¨è·‘å—", "status", setup="multi", tags=["multi"]),
    Scenario("multi_cancel", "killæŽ‰", "cancel", setup="multi", tags=["multi"]),
    Scenario("multi_task", "å¸®æˆ‘çœ‹çœ‹README", "task", setup="multi", tags=["multi"]),

    # â”€â”€ Edge cases â”€â”€
    Scenario("edge_short_ok", "ok", "task", tags=["edge"]),
    Scenario("edge_short_hmm", "å—¯", "task", tags=["edge"]),
    Scenario("edge_followup", "ç»§ç»­æ”¹é‚£ä¸ªæ–‡ä»¶", "task", tags=["edge"]),
    Scenario("edge_emoji_only", "ðŸ‘", "task", tags=["edge"]),
]

assert len(SCENARIOS) >= 50, f"Need 50+ scenarios, got {len(SCENARIOS)}"


async def run_scenario(scenario: Scenario) -> E2EResult:
    """Run one e2e scenario through the real dispatcher pipeline."""
    d = make_dispatcher()
    tg: MockTelegram = d.tg
    runner: MockRunner = d.runner

    # Setup context
    if scenario.setup == "running":
        add_running_session(d, "sims", "running data analysis", 300)
    elif scenario.setup == "multi":
        add_running_session(d, "sims", "running data analysis", 300)
        add_running_session(d, "cortex", "fixing dispatcher bug", 120)

    msg = make_message(scenario.message, reply_to=scenario.reply_to)

    t0 = time.monotonic()
    await d._on_message(msg)
    latency = (time.monotonic() - t0) * 1000

    # Wait a tiny bit for fire-and-forget tasks
    await asyncio.sleep(0.1)

    # Detect what route was taken
    actual = detect_route(d, tg, runner)

    # For idle scenarios: "task" means runner was invoked OR no response
    # (since mock runner is async and _handle_task spawns a background task)
    if scenario.setup == "idle" and scenario.expected == "task":
        # In idle mode, tasks go through _handle_task which spawns async.
        # Check if a task was spawned (d._tasks should have something)
        if d._tasks or runner.invocations:
            actual = "task"
        elif not tg.sent:
            actual = "task"  # nothing happened = would be task

    # For task scenarios with running sessions: the dispatcher will
    # either spawn a new task or queue behind running
    if scenario.expected == "task" and runner.invocations:
        actual = "task"
    # If dispatcher replied with queue message, it's still a task route
    if scenario.expected == "task" and tg.sent:
        for s in tg.sent:
            txt = s.get("text", "")
            if "ç­‰ä¸€ä¸ªå®Œæˆ" in txt or "ç­‰è¿™ä¸ªä»»åŠ¡è·‘å®Œ" in txt:
                actual = "task"

    passed = actual == scenario.expected
    details = ""
    if tg.sent:
        details = tg.sent[-1].get("text", "")[:120]
    elif runner.invocations:
        details = f"runner invoked: {runner.invocations[-1]['prompt_preview'][:80]}"

    # Cleanup spawned tasks
    for task in list(d._tasks):
        task.cancel()
        try:
            await task
        except (asyncio.CancelledError, Exception):
            pass

    return E2EResult(
        scenario=scenario.name,
        message=scenario.message,
        expected_route=scenario.expected,
        actual_route=actual,
        passed=passed,
        latency_ms=latency,
        details=details,
        tags=scenario.tags,
    )


async def main():
    print(f"Running {len(SCENARIOS)} e2e routing tests...\n")

    results: list[E2EResult] = []
    for i, scenario in enumerate(SCENARIOS, 1):
        r = await run_scenario(scenario)
        icon = "PASS" if r.passed else "FAIL"
        print(f"  [{i:2d}/{len(SCENARIOS)}] {icon}  {r.latency_ms:6.0f}ms  "
              f"expect={r.expected_route:8s} got={r.actual_route:8s}  "
              f"{r.scenario}: {r.message[:40]}")
        if not r.passed:
            print(f"           â†’ {r.details[:100]}")
        results.append(r)
        await asyncio.sleep(0.05)

    # Analysis
    total = len(results)
    passed = sum(1 for r in results if r.passed)
    failed = [r for r in results if not r.passed]

    print("\n" + "=" * 70)
    print("E2E ROUTING REPORT")
    print("=" * 70)
    print(f"\nOverall: {passed}/{total} ({passed/total*100:.1f}%)")

    latencies = [r.latency_ms for r in results]
    print(f"Latency: avg={sum(latencies)/len(latencies):.0f}ms "
          f"p50={sorted(latencies)[len(latencies)//2]:.0f}ms "
          f"max={max(latencies):.0f}ms")

    # Per-tag
    tags: dict[str, dict] = {}
    for r in results:
        for tag in r.tags:
            if tag not in tags:
                tags[tag] = {"total": 0, "passed": 0}
            tags[tag]["total"] += 1
            if r.passed:
                tags[tag]["passed"] += 1

    print("\nâ”€â”€ By category â”€â”€")
    for tag, stats in sorted(tags.items()):
        pct = stats["passed"] / stats["total"] * 100
        print(f"  {tag:12s}: {stats['passed']:2d}/{stats['total']:2d} ({pct:5.1f}%)")

    if failed:
        print(f"\nâ”€â”€ Failures ({len(failed)}) â”€â”€")
        for r in failed:
            print(f"  {r.scenario}: \"{r.message}\"")
            print(f"    expected={r.expected_route}, got={r.actual_route}")
            print(f"    details: {r.details[:100]}")

    # Save log
    log_path = Path(__file__).parent / "e2e_routing_log.json"
    log_data = {
        "total": total, "passed": passed,
        "accuracy": passed / total * 100,
        "results": [
            {
                "scenario": r.scenario, "message": r.message,
                "expected": r.expected_route, "got": r.actual_route,
                "passed": r.passed, "latency_ms": r.latency_ms,
                "details": r.details, "tags": r.tags,
            }
            for r in results
        ],
    }
    with open(log_path, "w") as f:
        json.dump(log_data, f, indent=2, ensure_ascii=False)
    print(f"\nLog saved to: {log_path}")
    print("=" * 70)

    return 0 if passed / total >= 0.90 else 1


if __name__ == "__main__":
    code = asyncio.run(main())
    sys.exit(code)
